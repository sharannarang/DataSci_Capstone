---
title: "Exploratory analysis for text prediction using NLP"
output: html_document
---
## Introduction

The Capstone Project involves using existing dataset to build text models and predict the next word in a given sentance. As a part of the capstone, the first step inolves exploring the dataset and getting familiar with textual data. This report performs exploratory analysis on the dataset and tokenizes the data. The dataset taken into account was the english dataset.

## Exploratory Analysis.
We start by exploring the size of the datasets in terms of bytes and number of lines
 

```{r load required libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(ggplot2)
library(gridExtra)
source("helper.R")
```

```{r read data, cache=TRUE, echo=FALSE, warning=FALSE}
blogs <- readLines("en_US.blogs.txt", encoding = "UTF-8")
twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8")
news <- readLines("clean_news.txt", encoding = "UTF-8")
```

```{r exploratory analysis, echo=FALSE, warning=FALSE}
chars.blogs <- nchar(blogs)
chars.twitter <- nchar(twitter)
chars.news <- nchar(news)

length.blogs <- length(blogs)
length.twitter <- length(twitter)
length.news <- length(news)

data.frame(DataSet=c("Blogs", "Twitter", "News"), Size=c(object.size(blogs), object.size(twitter), object.size(news)), Number.of.Lines=c(length.blogs, length.twitter, length.news))
```

Next, we look at the number of characters for each entry using a barplot. In order to represent the blogs and news datasets, y axis is log scaled. From the plots, it is evident that the blogs and news datasets have larger posts. Also, as expected, the max length of the twitter post is 140 charaters.


```{r expl analysis, echo=FALSE, warning=FALSE}
par(mfrow = c(1,3))
boxplot(chars.blogs, col="green", log="y")
title(main="Blogs", ylab="Log scale of length of entries")
boxplot(chars.news, col="blue", log="y")
title(main="News", ylab="Log scale of length of entries")
boxplot(chars.twitter, col="red")
title(main="Tweets", ylab="Length of entries")
```


## Cleaning the dataset
After loading the dataset, it is noticible that the data has a lot of bad charaters. Here are the steps taken to clean the dataset:

* Replace different types of apostrophes with the simple apostrophe
* Remove punctuation
* Remove numbers
* Remove '\\032' character that is not decoded correctly
* Reomve some other unicode characters which map to apostrophes and quotes
* Remove any whitespace
* Convert all characters to lowercase

The code for the cleaning function is present in the appendix.

#### Profanity filtering
One of the requirements of the project was to remove profanities from the datasets. There are several profanity lists available which could be used to filter all profanities. However, in a languague modelling task, removing the profanity may leave us with an incomplete sentance which would not be helpful in prediction. Therefore, we have decided to leave the profanities in the modelling task. One of the three approaches will be used to mask profanities from the resutls:

1. Ignore results with profanities
2. Replace profanity with another word of similar meaning
3. Display EXPLETIVE instead of the profanity in the results

## Unique words in the dataset
Using the cleaning approach described above, each dataset is cleaned and then split into seperate words. With the help of the unique function, we compute the unique words for each of the datasets and the combined dataset. 

```{r unique words, echo=FALSE}
load("words.RData")
unique.words <- unique(c(blogs_words, twitter_words, news_words))
data.frame(Dataset=c("Blogs", "News", "Twitter", "Combined"), UniqueWords = c(length(blogs_words), length(news_words), length(twitter_words), length(unique.words)))
```

There are approximately `r length(unique.words)` words with all the datasets combined.

## Developing the Corpus

#### Random Sampling
Storing `r length(unique.words)` words in the memory would be very expensive. In addition, it would be very time consuming and computationally expensive to to generate tokens for the entire dataset. Therefore, we have sampled each of the datsets to retain 20% of the original data. The sampling function is present in the Appendix. The sampled dataset is cleaned and written back to the disk.

The corpus is created by reading the cleaned and sampled dataset from the disk. 

## Tokenization

Document Term Matrices are computed for 1-gram, 2-gram, 3-gram and 4-gram matrices. The computation is done using RWeka and tm packages. Using the document term matrices, histograms are plotted to depict the counts of the ngrams. Logscale is used since there are a large number of words that show up only a few times in the dataset. From the plots, it is evident that several n-grams are observed only a few times in the dataset.

```{r load freqs, echo=FALSE}
load("freq_cts.RData")
```

```{r create hist, echo=FALSE, message=FALSE}
g1 <- plot_ngram_histogram(freq_uni,"1-gram counts")
g2 <- plot_ngram_histogram(freq_bi, "2-gram counts")
grid.arrange(g1,g2,nrow=2)
```


From the 1-gram matrix, we see that there are `r length(freq_uni)` unique terms in the corpus. This reprsents about `r round(length(freq_uni)/length(unique.words),5) * 100`% of the entire dataset. 

Using the matrices, we have computed the top 20 n-gram terms. The plots for each of the n-grams are shown below.

```{r load top ngrams, echo=FALSE}
load("popular_terms_data.RData")
```

```{r plot top ngrams, echo=FALSE}
plot_popular_terms(popular_terms_uni, 20, "1-gram top terms")
plot_popular_terms(popular_terms_bi, 20, "2-gram top terms")
plot_popular_terms(popular_terms_tri, 20, "3-gram top terms")
plot_popular_terms(popular_terms_quad, 20, "4-gram top terms")
```

From the 1-gram plot, it is evident that a lot of the words are stop words. In the plot below, we have eliminated all the stopwords and plot the top 15 non stop words.

```{r plot 1gram ns, echo=FALSE}
plot_popular_terms(popular_terms_uni_ns, 15, "1-gram non stop words terms")
```

## Modeling

N-grams will be used to develop the language model. From the Document Term Matrices, it is possible to create probability of the occurance of each n-gram. These probabilities can be used to predict the next word following an n-gram. In order to develop a complete model, we have to address the following questions:

##### Open Questions

* Stemming: Should the words be stemmed? 
* Apostrophe's: Is it good to remove all apostrophes? E.g Removing the apostrophe in Andy's might leave us with Anyds. 
* StopWords: Should we remove all stopwords from the dataset? 
* Sample Size: Will changing the sample size increase the accuracy dramatically
* N-grams: Would larger n-grams help prediction?
* Unknown words: Need a strategy to deal with unknown words. 

We will consider these questions as the machine learning algorithm is developed to imporove the accuracy of the predictions. 


## Apendix

__Sampling Function__
```{r sample funciton, echo=TRUE, eval=FALSE}
create_sample <- function(src, out, lines) {
    text <- readLines(src, lines)
    sub<- rbinom(lines, 1, 0.2)
    out.txt <- text[sub==1]
    writeLines(out.txt, out)
    out.txt
}

blogs <- create_sample("en_US.blogs.txt", "sample/blogs.txt", length(blogs))
news <- create_sample("en_US.news.txt", "sample/news.txt", length(news))
tweets <- create_sample("en_US.twitter.txt", "sample/twitter.txt",length(twitter))
```

__Cleaning function__
```{r cleaning funciton, echo=TRUE, eval=FALSE}
clean_up <- function(text) {
    clean.text <- gsub("\u2092", "'", text)
    clean.text <- gsub("\u2019", "'", clean.text)
    clean.text <- removePunctuation(clean.text)
    clean.text <- removeNumbers(clean.text)
    clean.text <- removePunctuation(gsub("\032", "", clean.text))
    clean.text <- gsub("\u0093|\u0092|\u0094", "", clean.text)
    clean.text <- stripWhitespace(clean.text)
    clean.text <- tolower(clean.text)
    clean.text
}
```

__Tokenization function__
```{r tokenization function, echo=TRUE, eval=FALSE}
## Tokenization
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))

## Create 1,2,3 gram tokens
dtm_uni <- DocumentTermMatrix(en.cor)
dtm_bi <- DocumentTermMatrix(en.cor, control = list(tokenize= BigramTokenizer))
dtm_tri <- DocumentTermMatrix(en.cor, control = list(tokenize= TrigramTokenizer))
dtm_quad <- DocumentTermMatrix(en.cor, control = list(tokenize= QuadgramTokenizer))
```