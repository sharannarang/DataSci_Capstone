---
title: "Exploratory analysis for text prediction using NLP"
output: html_document
---
## Introduction

The Capstone Project involves using existing dataset to build text models and predict the next word in a given sentance. As a part of the capstone, the first step inolves exploring the dataset and getting familiar with textual data. 

## Exploratory Analysis.

In order to explore the dataset, we start by reading in the data and constructing simple barplots. The barplots depict the length of entries in each of the datasets. In order to represent the blogs and news datasets, y axis is log scaled. From the plots, it is evident that the blogs and news datasets have larger posts. Also, as expected, the max length of the twitter post is 140 charaters. 

```{r load required libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(ggplot2)
```

```{r read data, cache=TRUE}
blogs <- readLines("en_US.blogs.txt", encoding = 'UTF-8')
twitter <- readLines("en_US.twitter.txt", encoding = 'UTF-8')
news <- readLines("en_US.news.txt", encoding = 'UTF-8')
```

```{r exploratory analysis, echo=TRUE, warning=FALSE}
chars.blogs <- nchar(blogs)
chars.twitter <- nchar(twitter)
chars.news <- nchar(news)

par(mfrow = c(1,3))
boxplot(chars.blogs, col="green", log="y")
title(main="Blogs", ylab="Log scale of length of entries")
boxplot(chars.news, col="blue", log="y")
title(main="News", ylab="Log scale of length of entries")
boxplot(chars.twitter, col="red")
title(main="Tweets", ylab="Length of entries")
```

```{r unique words, eval=FALSE}
blogs <- clean_up(blogs)
blogs.words <- unique(unlist(strsplit(blogs, " ")))
twitter <- clean_up(twitter)
twitter.words <- unique(unlist(strsplit(twitter, " ")))
news <- clean_up(blogs)
news.words <- unique(unlist(strsplit(news, " ")))

unique.words <- unique(c(blogs.words, twitter.words, news.words))

```

## Random Sampling

```{r sample funciton, echo=TRUE, eval=FALSE}
create_sample <- function (input_file, output_file, lines) {
  con1 <- file(input_file, 'r')
  con2 <- file(output_file, "w")
  sample <- readLines(con1, 1)
  for (i in 2:lines) {
    if (rbinom(1,1,0.2)) {
      sample <- c(sample,readLines(con=con1,n=1, encoding="UTF-8"))
    }
    else {
      gbg <- readLines(con1, 1)
    }
  }
  close(con1)
  
  writeLines(sample,con2)
  close(con2)    
  sample
}

## Pick random texts from the files
blogs <- create_sample("en_US.blogs.txt", "sample/blogs.txt", 899288)
news <- create_sample("en_US.news.txt", "sample/news.txt", 77259)
tweets <- create_sample("en_US.twitter.txt", "sample/twitter.txt",2360148)
```


## Developing the Corpus

```{r create corpus, echo=TRUE, cache=TRUE, eval=FALSE}
## Create the corpus
en.cor <- VCorpus(DirSource("clean_sample/"))
```

```{r clean the corpus, echo=TRUE, cache=TRUE, eval=FALSE}
en.cor<- tm_map(en.cor, tolower)
en.cor<- tm_map(en.cor, removeNumbers)
en.cor<- tm_map(en.cor, removePunctuation)
en.cor <- tm_map(en.cor, removeWords, stopwords("english"))
en.cor <- tm_map(en.cor, stripWhitespace)
en.cor <- tm_map(en.cor, PlainTextDocument)
```

## Tokenization

```{r Tokenization, echo=TRUE, cache=TRUE, message=FALSE}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
```

```{r load data, echo=FALSE}
load(".RData")
dtm_tri
```