---
title: "Exploratory analysis for text prediction using NLP"
output: html_document
---
## Introduction

The Capstone Project involves using existing dataset to build text models and predict the next word in a given sentance. As a part of the capstone, the first step inolves exploring the dataset and getting familiar with textual data. 

## Exploratory Analysis.

In order to explore the dataset, we start by reading in the data and constructing simple barplots. The barplots also d

```{r load required libraries, echo=FALSE, message=FALSE, warning=FALSE}
library(tm)
library(RWeka)
library(ggplot2)
```

```{r exploratory analysis, echo=TRUE, warning=FALSE}
chars.blogs <- nchar(readLines("en_US.blogs.txt", encoding = 'UTF-8'))
chars.twitter <- nchar(readLines("en_US.twitter.txt", encoding = 'UTF-8'))
chars.news <- nchar(readLines("en_US.news.txt", encoding = 'UTF-8'))

par(mfrow = c(1,3))
boxplot(chars.blogs, col="green", log="y")
title(main="Blogs", ylab="Log scale of length of entries")
boxplot(chars.news, col="blue", log="y")
title(main="News", ylab="Log scale of length of entries")
boxplot(chars.twitter, col="red")
title(main="Tweets", ylab="Length of entries")
```

## Random Sampling

```{r sample funciton, echo=TRUE, eval=FALSE}
create_sample <- function (input_file, output_file, lines) {
  con1 <- file(input_file, 'r')
  con2 <- file(output_file, "w")
  sample <- readLines(con1, 1)
  for (i in 2:lines) {
    if (rbinom(1,1,0.2)) {
      sample <- c(sample,readLines(con=con1,n=1, encoding="UTF-8"))
    }
    else {
      gbg <- readLines(con1, 1)
    }
  }
  close(con1)
  
  writeLines(sample,con2)
  close(con2)    
  sample
}

## Pick random texts from the files
blogs <- create_sample("en_US.blogs.txt", "sample/blogs.txt", 899288)
news <- create_sample("en_US.news.txt", "sample/news.txt", 77259)
tweets <- create_sample("en_US.twitter.txt", "sample/twitter.txt",2360148)
```


## Developing the Corpus

```{r create corpus, echo=TRUE, cache=TRUE}
## Create the corpus
en.cor <- VCorpus(DirSource("sample/"))
```

```{r clean the corpus, echo=TRUE, cache=TRUE}
en.cor<- tm_map(en.cor, tolower)
en.cor<- tm_map(en.cor, removeNumbers)
en.cor<- tm_map(en.cor, removePunctuation)
en.cor <- tm_map(en.cor, removeWords, stopwords("english"))
en.cor <- tm_map(en.cor, stripWhitespace)
en.cor <- tm_map(en.cor, PlainTextDocument)
```

## Tokenization

```{r Tokenization, echo=TRUE, cache=TRUE, message=FALSE}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
```
